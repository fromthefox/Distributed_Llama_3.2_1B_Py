{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f90027e-810d-455f-8c26-eb8d473dbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43a682c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = r\"C:\\Users\\yhbia\\Desktop\\学校\\∞-杂乱\\Llama_3.2_1B\\tokenizer.model\"\n",
    "special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3298af41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"tok_embeddings.weight\",\n",
      "    \"layers.0.attention.wq.weight\",\n",
      "    \"layers.0.attention.wk.weight\",\n",
      "    \"layers.0.attention.wv.weight\",\n",
      "    \"layers.0.attention.wo.weight\",\n",
      "    \"layers.0.feed_forward.w1.weight\",\n",
      "    \"layers.0.feed_forward.w3.weight\",\n",
      "    \"layers.0.feed_forward.w2.weight\",\n",
      "    \"layers.0.attention_norm.weight\",\n",
      "    \"layers.0.ffn_norm.weight\",\n",
      "    \"layers.1.attention.wq.weight\",\n",
      "    \"layers.1.attention.wk.weight\",\n",
      "    \"layers.1.attention.wv.weight\",\n",
      "    \"layers.1.attention.wo.weight\",\n",
      "    \"layers.1.feed_forward.w1.weight\",\n",
      "    \"layers.1.feed_forward.w3.weight\",\n",
      "    \"layers.1.feed_forward.w2.weight\",\n",
      "    \"layers.1.attention_norm.weight\",\n",
      "    \"layers.1.ffn_norm.weight\",\n",
      "    \"layers.2.attention.wq.weight\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(r\"C:\\Users\\yhbia\\Desktop\\学校\\∞-杂乱\\Llama_3.2_1B\\consolidated.00.pth\", map_location=\"cpu\")\n",
    "print(json.dumps(list(model.keys())[:20], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77e0e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\yhbia\\Desktop\\学校\\∞-杂乱\\Llama_3.2_1B\\params.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10acdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = config[\"dim\"]\n",
    "n_layers = config[\"n_layers\"]\n",
    "n_heads = config[\"n_heads\"]\n",
    "n_kv_heads = config[\"n_kv_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "multiple_of = config[\"multiple_of\"]\n",
    "ffn_dim_multiplier = config[\"ffn_dim_multiplier\"]\n",
    "norm_eps = config[\"norm_eps\"]\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d772790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 9514, 527, 264, 1732, 449]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"you are a person with\"\n",
    "tokens = [128000] + tokenizer.encode(prompt)\n",
    "print(tokens)\n",
    "tokens = torch.tensor(tokens)\n",
    "prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66cae73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = torch.nn.Embedding(vocab_size, dim)\n",
    "embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
    "token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baa1b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fa5cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_to_one_split_into_32_parts = torch.tensor(range(32))/32\n",
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_32_parts)\n",
    "input_token_length = len(prompt_split_as_tokens)\n",
    "freqs_for_each_token = torch.outer(torch.arange(input_token_length), freqs)\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2580efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = token_embeddings_unnormalized\n",
    "for layer in range(n_layers):\n",
    "    qkv_attention_store = []\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model[f\"layers.{layer}.attention_norm.weight\"])\n",
    "    q_layer = model[f\"layers.{layer}.attention.wq.weight\"]\n",
    "    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)\n",
    "    k_layer = model[f\"layers.{layer}.attention.wk.weight\"]\n",
    "    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)\n",
    "    v_layer = model[f\"layers.{layer}.attention.wv.weight\"]\n",
    "    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)\n",
    "    w_layer = model[f\"layers.{layer}.attention.wo.weight\"]\n",
    "    for head in range(n_heads):\n",
    "        q_layer_head = q_layer[head]\n",
    "        k_layer_head = k_layer[head//4]\n",
    "        v_layer_head = v_layer[head//4]\n",
    "        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)\n",
    "        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)\n",
    "        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)\n",
    "        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)\n",
    "        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)\n",
    "        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)\n",
    "        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)\n",
    "        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)\n",
    "        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)\n",
    "        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)\n",
    "        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)\n",
    "        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5\n",
    "        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(\"-inf\"))\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        qk_per_token_after_masking = qk_per_token + mask\n",
    "        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)\n",
    "        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)\n",
    "        qkv_attention_store.append(qkv_attention)\n",
    "\n",
    "    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)\n",
    "    w_layer = model[f\"layers.{layer}.attention.wo.weight\"]\n",
    "    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)\n",
    "    embedding_after_edit = final_embedding + embedding_delta\n",
    "    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f\"layers.{layer}.ffn_norm.weight\"])\n",
    "    w1 = model[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
    "    w2 = model[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
    "    w3 = model[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)\n",
    "    final_embedding = embedding_after_edit+output_after_feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c514d63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embedding = rms_norm(final_embedding, model[\"norm.weight\"])\n",
    "logits = torch.matmul(final_embedding[-1], model[\"output.weight\"].T)\n",
    "next_token = torch.argmax(logits, dim=-1)\n",
    "tokenizer.decode([next_token.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
